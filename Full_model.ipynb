{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import matplotlib.cm as cm\n",
    "from scipy.misc import imresize\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import scipy.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extraction(path):\n",
    "    data_l, data_r, data_all = [], [], []\n",
    "    nor_data = io.loadmat(path)\n",
    "    temp = nor_data['data']\n",
    "    r = temp['right']\n",
    "    l = temp['left']\n",
    "    r_temp = r[0,0]\n",
    "    l_temp = l[0,0]\n",
    "    data_r.append(r_temp['image'][0][0])\n",
    "    data_r.append(r_temp['gaze'][0][0])\n",
    "    data_r.append(r_temp['pose'][0][0])\n",
    "    data_l.append(l_temp['image'][0][0])\n",
    "    data_l.append(l_temp['gaze'][0][0])\n",
    "    data_l.append(l_temp['pose'][0][0])\n",
    "    for i in range(3):\n",
    "        data_all.append(data_r[i])   #0 -> r_img, 1-> l_img, 2-> r_gaze, 3->l_gaze..\n",
    "        data_all.append(data_l[i])\n",
    "    return data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_extraction(path, screen_path):\n",
    "    txt = pd.read_csv(path, sep = ' ', header= None)\n",
    "    screen = io.loadmat(screen_path)\n",
    "    df_anno = pd.DataFrame(txt)\n",
    "    data_frame = pd.DataFrame()\n",
    "    data_frame['Screen_x'] = df_anno[:][24] / screen['width_pixel'][0][0]\n",
    "    data_frame['Screen_y'] = df_anno[:][25] / screen['height_pixel'][0][0]\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def path_optimization(day_list, path):\n",
    "    all_path = []\n",
    "    cnt = 0\n",
    "    for i in range(len(day_list)): # 15번 \n",
    "        temp = []\n",
    "        for j in range(day_list[i]):\n",
    "            temp.append(path[cnt])\n",
    "            cnt += 1\n",
    "        all_path.append(temp)\n",
    "    return all_path\n",
    "\n",
    "day_list = [39, 69, 39, 65, 25, 38, 62, 56, 47, 20, 16, 19, 7, 12, 7]  \n",
    "data_path = glob('/home/spow12/Codes/Data/MPIIGaze/Data//Normalized/*/*')# 521개\n",
    "data_path.sort()    #Sort for prevent index error\n",
    "data_path = path_optimization(day_list, data_path)\n",
    "#data_path = path_optimization(glob('C:/MPIIGaze/MPIIGaze/Data/Normalized/*/*'))\n",
    "label_path = glob('/home/spow12/Codes/Data/MPIIGaze/Data/Original/*/*/annotation.txt') # 521개\n",
    "label_path.sort()\n",
    "label_path = path_optimization(day_list, label_path)\n",
    "#label_path = path_optimzation(glob('C:/MPIIGaze/MPIIGaze/Data/Original/*/*/annotation.txt'))\n",
    "screen_size_path = glob('/home/spow12/Codes/Data/MPIIGaze/Data/Original/*/Calibration/screenSize.mat') # 15개\n",
    "\n",
    "for i in range(15): # 총 15명\n",
    "    data = [] # 사람별로 나눈 데이터\n",
    "    label = [] # 사람별로 나눈 라벨   \n",
    "    for j in range(day_list[i]): # 각 사람에 맞는 day만큼\n",
    "        data.append(data_extraction(data_path[i][j]))\n",
    "        label.append(label_extraction(label_path[i][j], screen_size_path[i]))\n",
    "    exec('p%d_data = data' % (i))\n",
    "    exec('p%d_label = label' % (i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_img_label(image_shape, label_shape, data_list, label_list):\n",
    "    length = len(data_list[0])\n",
    "    left_img = np.zeros([length,image_shape])\n",
    "    right_img = np.zeros([length,image_shape])\n",
    "    label = np.zeros([length,label_shape])\n",
    "    for n in range(length):\n",
    "        left_img[n, :] = data_list[1][n].reshape(image_shape)      \n",
    "        right_img[n, :] = data_list[0][n].reshape(image_shape)\n",
    "        label[n, :] = [label_list['Screen_x'][n], label_list['Screen_y'][n]]\n",
    "        \n",
    "    return right_img, left_img, label\n",
    "\n",
    "def make_dataset_gaze_pose(data_shape, data_list):\n",
    "    length = len(data_list[0])\n",
    "    left_pose = np.zeros([length,data_shape])\n",
    "    right_pose = np.zeros([length,data_shape])\n",
    "    left_gaze = np.zeros([length,data_shape])\n",
    "    right_gaze = np.zeros([length,data_shape])\n",
    "    for n in range(length):\n",
    "        left_gaze[n, :] = data_list[3][n]\n",
    "        right_gaze[n, :] = data_list[2][n]\n",
    "        left_pose[n, :] = data_list[5][n]\n",
    "        right_pose[n, :] = data_list[4][n]\n",
    "    return left_gaze, right_gaze, left_pose, right_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_RE_img_data = []\n",
    "Batch_LE_img_data = []\n",
    "#Batch_LE_gaze_data = []\n",
    "#Batch_RE_gaze_data = [] \n",
    "#Batch_LE_pose_data = []\n",
    "#Batch_RE_pose_data = []\n",
    "Batch_label = []\n",
    "for i in range(15):\n",
    "    for j in range(day_list[i]): #days list in Data  po -> 39days  ..\n",
    "        exec('RE_img_data, LE_img_data, dataset_label = make_dataset_img_label(36 * 60, 2, p{0}_data[{1}], p{2}_label[{3}])'\n",
    "             .format(i, j, i, j))\n",
    "        #exec('LE_gaze_data, RE_gaze_data, LE_pose_data, RE_pose_data = make_dataset_gaze_pose(3, p{0}_data[{1}])'.format(i, j))\n",
    "        Batch_RE_img_data.append(RE_img_data)\n",
    "        Batch_LE_img_data.append(LE_img_data)\n",
    "#         Batch_LE_gaze_data.append(LE_gaze_data)\n",
    "#         Batch_RE_gaze_data.append(RE_gaze_data)\n",
    "#         Batch_LE_pose_data.append(LE_pose_data)\n",
    "#         Batch_RE_pose_data.append(RE_pose_data)\n",
    "        Batch_label.append(dataset_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make graph for eye images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_for_image(X_img, keep_prob, name):\n",
    "    with tf.name_scope(\"Image_layer_1\" + name) as scope:\n",
    "        W1 = tf.Variable(tf.random_normal([4, 4, 1, 5], stddev=0.01))\n",
    "        L1 = tf.nn.conv2d(X_img, W1, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        L1 = tf.nn.relu(L1)\n",
    "\n",
    "    with tf.name_scope(\"Image_layer_2\" + name) as scope:\n",
    "        W2 = tf.Variable(tf.random_normal([2, 2, 5, 10], stddev=0.01))\n",
    "        L2 = tf.nn.conv2d(L1, W2, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        L2 = tf.nn.relu(L2)\n",
    "        L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "        L2_flat = tf.reshape(L2, [-1, 5 * 8 * 10])\n",
    "    \n",
    "\n",
    "    with tf.name_scope(\"Image_layer_3\" + name) as scope:\n",
    "        W3 = tf.get_variable(\"W3_\"+name ,shape= [400, 256],\n",
    "                             initializer= tf.contrib.layers.xavier_initializer())\n",
    "        b3 = tf.Variable(tf.random_normal([256]))\n",
    "        L3 = tf.nn.relu(tf.matmul(L2_flat, W3) + b3)\n",
    "        L3 = tf.nn.dropout(L3, keep_prob= keep_prob)\n",
    "\n",
    "    with tf.name_scope(\"Image_layer_4\" + name) as scope:\n",
    "        W4 = tf.get_variable(\"W4_\"+name ,shape= [256, 128],\n",
    "                             initializer= tf.contrib.layers.xavier_initializer())\n",
    "        b4 = tf.Variable(tf.random_normal([128]))\n",
    "        L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "        L4 = tf.nn.dropout(L4, keep_prob= keep_prob)\n",
    "    \n",
    "    with tf.name_scope(\"Image_layer_5\" + name) as scope:\n",
    "        W5 = tf.get_variable(\"W5_\"+name ,shape= [128, 64],\n",
    "                             initializer= tf.contrib.layers.xavier_initializer())\n",
    "        b5 = tf.Variable(tf.random_normal([64]))\n",
    "        logits = tf.matmul(L4, W5) + b5\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make graph for other datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_for_others(X, name):\n",
    "    with tf.name_scope(\"Layer_1\" + name) as scope:\n",
    "        W1 = tf.get_variable(\"W1_\"+name ,shape= [3, 3],\n",
    "                             initializer= tf.contrib.layers.xavier_initializer())\n",
    "        b1 = tf.Variable(tf.random_normal([3]))\n",
    "        L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    with tf.name_scope(\"Layer_2\" + name) as scope:\n",
    "        W2 = tf.get_variable(\"W2_\"+name ,shape= [3, 3],\n",
    "                             initializer= tf.contrib.layers.xavier_initializer())\n",
    "        b2 = tf.Variable(tf.random_normal([3]))\n",
    "        logits = tf.matmul(L1, W2) + b2\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variable and make graphs using function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0912 00:27:09.851532 140313676670720 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0912 00:27:09.861533 140313676670720 deprecation.py:506] From <ipython-input-7-dc5994c33a26>:21: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "Y = tf.placeholder(tf.float32, [None, 2])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "X_left = tf.placeholder(tf.float32, [None, 2160])\n",
    "X_right = tf.placeholder(tf.float32, [None, 2160])\n",
    "X_img_left = tf.reshape(X_left, [-1, 36, 60, 1])   \n",
    "X_img_right = tf.reshape(X_right, [-1, 36, 60, 1]) \n",
    "\n",
    "# X_gaze_r = tf.placeholder(tf.float32, [None, 3])\n",
    "# X_gaze_l = tf.placeholder(tf.float32, [None, 3])\n",
    "# X_pose_r = tf.placeholder(tf.float32, [None, 3])\n",
    "# X_pose_l = tf.placeholder(tf.float32, [None, 3])\n",
    "\n",
    "LE_img = model_for_image(X_img_left, keep_prob, 'left_eye')  #left eye imgae\n",
    "RE_img = model_for_image(X_img_right, keep_prob, 'right_eye')#right eye image\n",
    "# LE_pose = model_for_others(X_pose_l, 'left_pose')\n",
    "# LE_gaze = model_for_others(X_gaze_l, 'left_gaze')\n",
    "# RE_pose = model_for_others(X_pose_r, 'right_pose')\n",
    "# RE_gaze = model_for_others(X_gaze_r, 'right_gaze')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate the gaze graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope(\"Layer_gaze_merged\") as scope:\n",
    "#     LE_gaze_W = tf.get_variable(\"LE_gaze_W\" ,shape= [3, 3],\n",
    "#                              initializer= tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "#     RE_gaze_W = tf.get_variable(\"RE_gaze_W\" ,shape= [3, 3],\n",
    "#                              initializer= tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "#     layer_gaze = tf.nn.relu(tf.matmul(LE_gaze, LE_gaze_W) + tf.matmul(RE_gaze, RE_gaze_W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate the pose layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope(\"Layer_pose_merged\") as scope:\n",
    "#     LE_pose_W = tf.get_variable(\"LE_pose_W\" ,shape= [3, 3],\n",
    "#                              initializer= tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "#     RE_pose_W = tf.get_variable(\"RE_pose_W\" ,shape= [3, 3],\n",
    "#                              initializer= tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "#     layer_pose = tf.nn.relu(tf.matmul(LE_pose, LE_pose_W) + tf.matmul(RE_pose, RE_pose_W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate the Image layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Layer_Image_merged_1\") as scope:\n",
    "    LE_W_img = tf.get_variable(\"LE_W_img\" ,shape= [64, 32],\n",
    "                             initializer= tf.contrib.layers.xavier_initializer())\n",
    "    RE_W_img = tf.get_variable(\"RE_W_img\" ,shape= [64, 32],\n",
    "                             initializer= tf.contrib.layers.xavier_initializer())\n",
    "    layer_img = tf.nn.relu(tf.matmul(LE_img, LE_W_img) + tf.matmul(RE_img, RE_W_img))\n",
    "\n",
    "with tf.name_scope(\"Layer_Image_merged_2\") as scope:\n",
    "    img_W = tf.get_variable(\"img_W\" ,shape= [32, 16],\n",
    "                             initializer= tf.contrib.layers.xavier_initializer())\n",
    "    img_b = tf.Variable(tf.random_normal([16]))\n",
    "    layer2_img = tf.nn.relu(tf.matmul(layer_img, img_W) + img_b)\n",
    "\n",
    "with tf.name_scope(\"Layer_Image_merged_3\") as scope:\n",
    "    img_W2 = tf.get_variable(\"img_W2\" ,shape= [16, 8],\n",
    "                             initializer= tf.contrib.layers.xavier_initializer())\n",
    "    img_b2 = tf.Variable(tf.random_normal([8]))\n",
    "    layer3_img= tf.nn.relu(tf.matmul(layer2_img, img_W2) + img_b2)\n",
    "\n",
    "with tf.name_scope(\"Layer_Image_merged_Last\") as scope:\n",
    "#     Final_W_img = tf.get_variable(\"Final_W_img\" ,shape= [8, 3],\n",
    "#                              initializer= tf.contrib.layers.xavier_initializer())\n",
    "#     Final_b_img = tf.Variable(tf.random_normal([3]))\n",
    "#     Final_layer_img= tf.nn.relu(tf.matmul(layer3_img, Final_W_img) + Final_b_img)\n",
    "    Final_W_img = tf.get_variable(\"Final_W_img\" ,shape= [8, 2],\n",
    "                             initializer= tf.contrib.layers.xavier_initializer())\n",
    "    Final_b_img = tf.Variable(tf.random_normal([2]))\n",
    "    logits =  tf.matmul(layer3_img, Final_W_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope(\"Layer\") as scope:\n",
    "#     # Weight and bias for img layer\n",
    "#     All_W_img = tf.get_variable(\"All_W_img\" ,shape= [3, 2],\n",
    "#                              initializer= tf.contrib.layers.xavier_initializer())\n",
    "#     # Weight and bias for pose layer\n",
    "#     All_W_pose = tf.get_variable(\"All_W_pose\" ,shape= [3, 2],\n",
    "#                              initializer= tf.contrib.layers.xavier_initializer())\n",
    "#     # Weight and bias for gaze layer\n",
    "#     All_W_gaze = tf.get_variable(\"All_W_gaze\" ,shape= [3, 2],\n",
    "#                              initializer= tf.contrib.layers.xavier_initializer())\n",
    "#     logits = (tf.matmul(Final_layer_img, All_W_img) \n",
    "#               + tf.matmul(layer_pose, All_W_pose)\n",
    "#               + tf.matmul(layer_gaze, All_W_gaze))\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = tf.reduce_mean(tf.square(logits-Y))\n",
    "\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "summary = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started.\n",
      "step: 0, cost: 0.7690896987915039\n",
      "step: 1, cost: 0.46866631507873535\n",
      "step: 2, cost: 0.2511560916900635\n",
      "step: 3, cost: 0.4830506443977356\n",
      "step: 4, cost: 0.29519417881965637\n",
      "step: 5, cost: 0.25252676010131836\n",
      "step: 6, cost: 0.24574269354343414\n",
      "step: 7, cost: 0.2780194580554962\n",
      "step: 8, cost: 0.2571008801460266\n",
      "step: 9, cost: 0.22935278713703156\n",
      "step: 10, cost: 0.2190808653831482\n",
      "step: 11, cost: 0.20605072379112244\n",
      "step: 12, cost: 0.19693447649478912\n",
      "step: 13, cost: 0.22212575376033783\n",
      "step: 14, cost: 0.20544682443141937\n",
      "step: 15, cost: 0.21702444553375244\n",
      "step: 16, cost: 0.2017958164215088\n",
      "step: 17, cost: 0.194117471575737\n",
      "step: 18, cost: 0.17154739797115326\n",
      "step: 19, cost: 0.20084431767463684\n",
      "step: 20, cost: 0.1968308836221695\n",
      "step: 21, cost: 0.1990969181060791\n",
      "step: 22, cost: 0.18175657093524933\n",
      "step: 23, cost: 0.1864393651485443\n",
      "step: 24, cost: 0.17940406501293182\n",
      "step: 25, cost: 0.1758061647415161\n",
      "step: 26, cost: 0.17980556190013885\n",
      "step: 27, cost: 0.17610950767993927\n",
      "step: 28, cost: 0.18225505948066711\n",
      "step: 29, cost: 0.17991048097610474\n",
      "step: 30, cost: 0.17735342681407928\n",
      "step: 31, cost: 0.17135223746299744\n",
      "step: 32, cost: 0.1714736670255661\n",
      "step: 33, cost: 0.16201604902744293\n",
      "step: 34, cost: 0.174795001745224\n",
      "step: 35, cost: 0.16102367639541626\n",
      "step: 36, cost: 0.16246166825294495\n",
      "step: 37, cost: 0.17082452774047852\n",
      "step: 38, cost: 0.16759946942329407\n",
      "step: 39, cost: 0.20920370519161224\n",
      "step: 40, cost: 0.21909470856189728\n",
      "step: 41, cost: 0.202541321516037\n",
      "step: 42, cost: 0.206570103764534\n",
      "step: 43, cost: 0.1566939800977707\n",
      "step: 44, cost: 0.19706103205680847\n",
      "step: 45, cost: 0.19413164258003235\n",
      "step: 46, cost: 0.20506317913532257\n",
      "step: 47, cost: 0.1959199756383896\n",
      "step: 48, cost: 0.20310425758361816\n",
      "step: 49, cost: 0.21099138259887695\n",
      "step: 50, cost: 0.1986519992351532\n",
      "step: 51, cost: 0.1985567808151245\n",
      "step: 52, cost: 0.1796649992465973\n",
      "step: 53, cost: 0.19088922441005707\n",
      "step: 54, cost: 0.18609829246997833\n",
      "step: 55, cost: 0.19185224175453186\n",
      "step: 56, cost: 0.16424904763698578\n",
      "step: 57, cost: 0.18537750840187073\n",
      "step: 58, cost: 0.1696864664554596\n",
      "step: 59, cost: 0.17534740269184113\n",
      "step: 60, cost: 0.18117892742156982\n",
      "step: 61, cost: 0.17895391583442688\n",
      "step: 62, cost: 0.1585472673177719\n",
      "step: 63, cost: 0.17255732417106628\n",
      "step: 64, cost: 0.12874136865139008\n",
      "step: 65, cost: 0.17645037174224854\n",
      "step: 66, cost: 0.1692340075969696\n",
      "step: 67, cost: 0.16844981908798218\n",
      "step: 68, cost: 0.18483448028564453\n",
      "step: 69, cost: 0.1538810282945633\n",
      "step: 70, cost: 0.15626387298107147\n",
      "step: 71, cost: 0.16412346065044403\n",
      "step: 72, cost: 0.17349030077457428\n",
      "step: 73, cost: 0.15566129982471466\n",
      "step: 74, cost: 0.17186026275157928\n",
      "step: 75, cost: 0.1469556987285614\n",
      "step: 76, cost: 0.16992303729057312\n",
      "step: 77, cost: 0.10916338860988617\n",
      "step: 78, cost: 0.15166887640953064\n",
      "step: 79, cost: 0.15601904690265656\n",
      "step: 80, cost: 0.17350119352340698\n",
      "step: 81, cost: 0.15019851922988892\n",
      "step: 82, cost: 0.17461805045604706\n",
      "step: 83, cost: 0.15085215866565704\n",
      "step: 84, cost: 0.1527261883020401\n",
      "step: 85, cost: 0.1665966957807541\n",
      "step: 86, cost: 0.16479267179965973\n",
      "step: 87, cost: 0.149004265666008\n",
      "step: 88, cost: 0.16377057135105133\n",
      "step: 89, cost: 0.1513908952474594\n",
      "step: 90, cost: 0.15828485786914825\n",
      "step: 91, cost: 0.15424685180187225\n",
      "step: 92, cost: 0.1589524894952774\n",
      "step: 93, cost: 0.1556168645620346\n",
      "step: 94, cost: 0.15094107389450073\n",
      "step: 95, cost: 0.14925028383731842\n",
      "step: 96, cost: 0.14907732605934143\n",
      "step: 97, cost: 0.13888542354106903\n",
      "step: 98, cost: 0.1394367814064026\n",
      "step: 99, cost: 0.1422482430934906\n",
      "step: 100, cost: 0.13067755103111267\n",
      "step: 101, cost: 0.14162196218967438\n",
      "step: 102, cost: 0.1533321589231491\n",
      "step: 103, cost: 0.1420353204011917\n",
      "step: 104, cost: 0.14430101215839386\n",
      "step: 105, cost: 0.1351650506258011\n",
      "step: 106, cost: 0.10902514308691025\n",
      "step: 107, cost: 0.17271877825260162\n",
      "step: 108, cost: 0.12793035805225372\n",
      "step: 109, cost: 0.1162809357047081\n",
      "step: 110, cost: 0.12879931926727295\n",
      "step: 111, cost: 0.11603328585624695\n",
      "step: 112, cost: 0.1164398193359375\n",
      "step: 113, cost: 0.11603017151355743\n",
      "step: 114, cost: 0.11579948663711548\n",
      "step: 115, cost: 0.12173960357904434\n",
      "step: 116, cost: 0.12480378895998001\n",
      "step: 117, cost: 0.11641257256269455\n",
      "step: 118, cost: 0.1216634139418602\n",
      "step: 119, cost: 0.12493989616632462\n",
      "step: 120, cost: 0.11972512304782867\n",
      "step: 121, cost: 0.11268729716539383\n",
      "step: 122, cost: 0.10891515761613846\n",
      "step: 123, cost: 0.11453893035650253\n",
      "step: 124, cost: 0.10941822826862335\n",
      "step: 125, cost: 0.10736989229917526\n",
      "step: 126, cost: 0.10248876363039017\n",
      "step: 127, cost: 0.11038629710674286\n",
      "step: 128, cost: 0.11817266792058945\n",
      "step: 129, cost: 0.10970568656921387\n",
      "step: 130, cost: 0.11102405935525894\n",
      "step: 131, cost: 0.10498767346143723\n",
      "step: 132, cost: 0.10834656655788422\n",
      "step: 133, cost: 0.10099849104881287\n",
      "step: 134, cost: 0.09207521378993988\n",
      "step: 135, cost: 0.10770133137702942\n",
      "step: 136, cost: 0.10234186053276062\n",
      "step: 137, cost: 0.10153317451477051\n",
      "step: 138, cost: 0.10257292538881302\n",
      "step: 139, cost: 0.09711496531963348\n",
      "step: 140, cost: 0.09714026749134064\n",
      "step: 141, cost: 0.09149593114852905\n",
      "step: 142, cost: 0.0994153693318367\n",
      "step: 143, cost: 0.0928693413734436\n",
      "step: 144, cost: 0.09441214054822922\n",
      "step: 145, cost: 0.08996913582086563\n",
      "step: 146, cost: 0.08335991203784943\n",
      "step: 147, cost: 0.10718554258346558\n",
      "step: 148, cost: 0.11011004447937012\n",
      "step: 149, cost: 0.11044039577245712\n",
      "step: 150, cost: 0.11337379366159439\n",
      "step: 151, cost: 0.09801366180181503\n",
      "step: 152, cost: 0.1061178594827652\n",
      "step: 153, cost: 0.1028628721833229\n",
      "step: 154, cost: 0.09302109479904175\n",
      "step: 155, cost: 0.0796150490641594\n",
      "step: 156, cost: 0.09954840689897537\n",
      "step: 157, cost: 0.10218936204910278\n",
      "step: 158, cost: 0.09181809425354004\n",
      "step: 159, cost: 0.10348684340715408\n",
      "step: 160, cost: 0.09892423450946808\n",
      "step: 161, cost: 0.10690858215093613\n",
      "step: 162, cost: 0.0892505869269371\n",
      "step: 163, cost: 0.10587753355503082\n",
      "step: 164, cost: 0.10131817311048508\n",
      "step: 165, cost: 0.09975876659154892\n",
      "step: 166, cost: 0.10491681843996048\n",
      "step: 167, cost: 0.09997217357158661\n",
      "step: 168, cost: 0.09380676597356796\n",
      "step: 169, cost: 0.09434868395328522\n",
      "step: 170, cost: 0.09199053049087524\n",
      "step: 171, cost: 0.08418501168489456\n",
      "step: 172, cost: 0.0875803604722023\n",
      "step: 173, cost: 0.09200245887041092\n",
      "step: 174, cost: 0.09130680561065674\n",
      "step: 175, cost: 0.09314809739589691\n",
      "step: 176, cost: 0.08163481205701828\n",
      "step: 177, cost: 0.08046643435955048\n",
      "step: 178, cost: 0.08238392323255539\n",
      "step: 179, cost: 0.0845969095826149\n",
      "step: 180, cost: 0.10841059684753418\n",
      "step: 181, cost: 0.08932539075613022\n",
      "step: 182, cost: 0.08139225095510483\n",
      "step: 183, cost: 0.07702882587909698\n",
      "step: 184, cost: 0.08417417854070663\n",
      "step: 185, cost: 0.089201420545578\n",
      "step: 186, cost: 0.08475746959447861\n",
      "step: 187, cost: 0.08464035391807556\n",
      "step: 188, cost: 0.08701963722705841\n",
      "step: 189, cost: 0.09826056659221649\n",
      "step: 190, cost: 0.09507770836353302\n",
      "step: 191, cost: 0.09268594533205032\n",
      "step: 192, cost: 0.09017988294363022\n",
      "step: 193, cost: 0.09428316354751587\n",
      "step: 194, cost: 0.0855773389339447\n",
      "step: 195, cost: 0.08316335827112198\n",
      "step: 196, cost: 0.08115380257368088\n",
      "step: 197, cost: 0.07623903453350067\n",
      "step: 198, cost: 0.0838128924369812\n",
      "step: 199, cost: 0.08258473128080368\n",
      "step: 200, cost: 0.08825485408306122\n",
      "step: 201, cost: 0.08248919248580933\n",
      "step: 202, cost: 0.0823681503534317\n",
      "step: 203, cost: 0.08316757529973984\n",
      "step: 204, cost: 0.08115425705909729\n",
      "step: 205, cost: 0.07494417577981949\n",
      "step: 206, cost: 0.08314330875873566\n",
      "step: 207, cost: 0.08194416761398315\n",
      "step: 208, cost: 0.08190710097551346\n",
      "step: 209, cost: 0.08743195980787277\n",
      "step: 210, cost: 0.07960540801286697\n",
      "step: 211, cost: 0.07860920578241348\n",
      "step: 212, cost: 0.06966034322977066\n",
      "step: 213, cost: 0.07665635645389557\n",
      "step: 214, cost: 0.10514980554580688\n",
      "step: 215, cost: 0.07707647979259491\n",
      "step: 216, cost: 0.09232497960329056\n",
      "step: 217, cost: 0.06972671300172806\n",
      "step: 218, cost: 0.07485294342041016\n",
      "step: 219, cost: 0.07827851921319962\n",
      "step: 220, cost: 0.07997652143239975\n",
      "step: 221, cost: 0.07523422688245773\n",
      "step: 222, cost: 0.07333424687385559\n",
      "step: 223, cost: 0.06915560364723206\n",
      "step: 224, cost: 0.06287777423858643\n",
      "step: 225, cost: 0.07601302117109299\n",
      "step: 226, cost: 0.075262650847435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 227, cost: 0.06935308128595352\n",
      "step: 228, cost: 0.07609367370605469\n",
      "step: 229, cost: 0.07250401377677917\n",
      "step: 230, cost: 0.06999155879020691\n",
      "step: 231, cost: 0.06724565476179123\n",
      "step: 232, cost: 0.08293311297893524\n",
      "step: 233, cost: 0.0648312121629715\n",
      "step: 234, cost: 0.06680280715227127\n",
      "step: 235, cost: 0.06466595083475113\n",
      "step: 236, cost: 0.060910556465387344\n",
      "step: 237, cost: 0.0938713327050209\n",
      "step: 238, cost: 0.09621425718069077\n",
      "step: 239, cost: 0.09265151619911194\n",
      "step: 240, cost: 0.1077377125620842\n",
      "step: 241, cost: 0.09509018063545227\n",
      "step: 242, cost: 0.07693219184875488\n",
      "step: 243, cost: 0.08783073723316193\n",
      "step: 244, cost: 0.08222126215696335\n",
      "step: 245, cost: 0.08672964572906494\n",
      "step: 246, cost: 0.08115910738706589\n",
      "step: 247, cost: 0.08034137636423111\n",
      "step: 248, cost: 0.08227182924747467\n",
      "step: 249, cost: 0.08491094410419464\n",
      "step: 250, cost: 0.08505649864673615\n",
      "step: 251, cost: 0.08007843792438507\n",
      "step: 252, cost: 0.07729148864746094\n",
      "step: 253, cost: 0.07541247457265854\n",
      "step: 254, cost: 0.08053156733512878\n",
      "step: 255, cost: 0.07506400346755981\n",
      "step: 256, cost: 0.08233284205198288\n",
      "step: 257, cost: 0.0790489986538887\n",
      "step: 258, cost: 0.0835346207022667\n",
      "step: 259, cost: 0.07087115198373795\n",
      "step: 260, cost: 0.07690608501434326\n",
      "step: 261, cost: 0.07727865874767303\n",
      "step: 262, cost: 0.07844977080821991\n",
      "step: 263, cost: 0.07146953046321869\n",
      "step: 264, cost: 0.0653000995516777\n",
      "step: 265, cost: 0.08817271143198013\n",
      "step: 266, cost: 0.06958220154047012\n",
      "step: 267, cost: 0.07198172807693481\n",
      "step: 268, cost: 0.08341827988624573\n",
      "step: 269, cost: 0.07809168845415115\n",
      "step: 270, cost: 0.08989521861076355\n",
      "step: 271, cost: 0.06630118936300278\n",
      "step: 272, cost: 0.0648675262928009\n",
      "step: 273, cost: 0.061227019876241684\n",
      "step: 274, cost: 0.07136794924736023\n",
      "step: 275, cost: 0.12843726575374603\n",
      "step: 276, cost: 0.12490533292293549\n",
      "step: 277, cost: 0.13074451684951782\n",
      "step: 278, cost: 0.14619484543800354\n",
      "step: 279, cost: 0.16382907330989838\n",
      "step: 280, cost: 0.12876054644584656\n",
      "step: 281, cost: 0.13102424144744873\n",
      "step: 282, cost: 0.14692579209804535\n",
      "step: 283, cost: 0.1446133255958557\n",
      "step: 284, cost: 0.11643856018781662\n",
      "step: 285, cost: 0.12499946355819702\n",
      "step: 286, cost: 0.12371250241994858\n",
      "step: 287, cost: 0.15614183247089386\n",
      "step: 288, cost: 0.11167929321527481\n",
      "step: 289, cost: 0.11341498792171478\n",
      "step: 290, cost: 0.10687229037284851\n",
      "step: 291, cost: 0.1091480627655983\n",
      "step: 292, cost: 0.1193741112947464\n",
      "step: 293, cost: 0.12216740846633911\n",
      "step: 294, cost: 0.11699134111404419\n",
      "step: 295, cost: 0.13204745948314667\n",
      "step: 296, cost: 0.11656896024942398\n",
      "step: 297, cost: 0.10966956615447998\n",
      "step: 298, cost: 0.13105499744415283\n",
      "step: 299, cost: 0.11805487424135208\n",
      "step: 300, cost: 0.14380130171775818\n",
      "step: 301, cost: 0.1231432855129242\n",
      "step: 302, cost: 0.10867923498153687\n",
      "step: 303, cost: 0.12842342257499695\n",
      "step: 304, cost: 0.13756370544433594\n",
      "step: 305, cost: 0.12553450465202332\n",
      "step: 306, cost: 0.11656517535448074\n",
      "step: 307, cost: 0.11886215209960938\n",
      "step: 308, cost: 0.13856713473796844\n",
      "step: 309, cost: 0.13649463653564453\n",
      "step: 310, cost: 0.12432381510734558\n",
      "step: 311, cost: 0.06664730608463287\n",
      "step: 312, cost: 0.10192414373159409\n",
      "step: 313, cost: 0.11900218576192856\n",
      "step: 314, cost: 0.13217774033546448\n",
      "step: 315, cost: 0.10619834065437317\n",
      "step: 316, cost: 0.12341111153364182\n",
      "step: 317, cost: 0.09814681112766266\n",
      "step: 318, cost: 0.11919954419136047\n",
      "step: 319, cost: 0.11860208213329315\n",
      "step: 320, cost: 0.10752175748348236\n",
      "step: 321, cost: 0.1034664586186409\n",
      "step: 322, cost: 0.10609926283359528\n",
      "step: 323, cost: 0.12395454943180084\n",
      "step: 324, cost: 0.11238819360733032\n",
      "step: 325, cost: 0.12413034588098526\n",
      "step: 326, cost: 0.09424326568841934\n",
      "step: 327, cost: 0.0922774001955986\n",
      "step: 328, cost: 0.09406735002994537\n",
      "step: 329, cost: 0.1103304997086525\n",
      "step: 330, cost: 0.10687306523323059\n",
      "step: 331, cost: 0.11057624220848083\n",
      "step: 332, cost: 0.08418538421392441\n",
      "step: 333, cost: 0.10069388151168823\n",
      "step: 334, cost: 0.11091756820678711\n",
      "step: 335, cost: 0.10272548347711563\n",
      "step: 336, cost: 0.09651794284582138\n",
      "step: 337, cost: 0.10016241669654846\n",
      "step: 338, cost: 0.15215592086315155\n",
      "step: 339, cost: 0.07914956659078598\n",
      "step: 340, cost: 0.09557781368494034\n",
      "step: 341, cost: 0.118569016456604\n",
      "step: 342, cost: 0.14039090275764465\n",
      "step: 343, cost: 0.08985790610313416\n",
      "step: 344, cost: 0.09036528319120407\n",
      "step: 345, cost: 0.08369579166173935\n",
      "step: 346, cost: 0.09444738924503326\n",
      "step: 347, cost: 0.13231348991394043\n",
      "step: 348, cost: 0.12753356993198395\n",
      "step: 349, cost: 0.12426233291625977\n",
      "step: 350, cost: 0.13273032009601593\n",
      "step: 351, cost: 0.18509259819984436\n",
      "step: 352, cost: 0.0854642391204834\n",
      "step: 353, cost: 0.1277170330286026\n",
      "step: 354, cost: 0.1115209087729454\n",
      "step: 355, cost: 0.15062038600444794\n",
      "step: 356, cost: 0.0946185290813446\n",
      "step: 357, cost: 0.16519644856452942\n",
      "step: 358, cost: 0.0829506441950798\n",
      "step: 359, cost: 0.160716712474823\n",
      "step: 360, cost: 0.11049477010965347\n",
      "step: 361, cost: 0.09552689641714096\n",
      "step: 362, cost: 0.09837087988853455\n",
      "step: 363, cost: 0.09965480118989944\n",
      "step: 364, cost: 0.0883227288722992\n",
      "step: 365, cost: 0.08053798973560333\n",
      "step: 366, cost: 0.09429619461297989\n",
      "step: 367, cost: 0.0779811441898346\n",
      "step: 368, cost: 0.0977444276213646\n",
      "step: 369, cost: 0.11005422472953796\n",
      "step: 370, cost: 0.1073710098862648\n",
      "step: 371, cost: 0.0807342529296875\n",
      "step: 372, cost: 0.08357701450586319\n",
      "step: 373, cost: 0.08759533613920212\n",
      "step: 374, cost: 0.1448640674352646\n",
      "step: 375, cost: 0.0969853475689888\n",
      "step: 376, cost: 0.08561800420284271\n",
      "step: 377, cost: 0.08504112809896469\n",
      "step: 378, cost: 0.09939342737197876\n",
      "step: 379, cost: 0.08731542527675629\n",
      "step: 380, cost: 0.07895180583000183\n",
      "step: 381, cost: 0.1159127801656723\n",
      "step: 382, cost: 0.13391247391700745\n",
      "step: 383, cost: 0.14263536036014557\n",
      "step: 384, cost: 0.08940526098012924\n",
      "step: 385, cost: 0.08983979374170303\n",
      "step: 386, cost: 0.12978297472000122\n",
      "step: 387, cost: 0.11784318089485168\n",
      "step: 388, cost: 0.09935799986124039\n",
      "step: 389, cost: 0.11319099366664886\n",
      "step: 390, cost: 0.0883922427892685\n",
      "step: 391, cost: 0.10606894642114639\n",
      "step: 392, cost: 0.13483871519565582\n",
      "step: 393, cost: 0.05238160490989685\n",
      "step: 394, cost: 0.0504789724946022\n",
      "step: 395, cost: 0.06621863692998886\n",
      "step: 396, cost: 0.05409081652760506\n",
      "step: 397, cost: 0.05875636264681816\n",
      "step: 398, cost: 0.059455644339323044\n",
      "step: 399, cost: 0.05760062485933304\n",
      "step: 400, cost: 0.04451317340135574\n",
      "step: 401, cost: 0.06334198266267776\n",
      "step: 402, cost: 0.06034260615706444\n",
      "step: 403, cost: 0.05826358124613762\n",
      "step: 404, cost: 0.053563348948955536\n",
      "step: 405, cost: 0.05133168771862984\n",
      "step: 406, cost: 0.05880521237850189\n",
      "step: 407, cost: 0.051846593618392944\n",
      "step: 408, cost: 0.05528099089860916\n",
      "step: 409, cost: 0.043306343257427216\n",
      "step: 410, cost: 0.053646303713321686\n",
      "step: 411, cost: 0.05332136154174805\n",
      "step: 412, cost: 0.05591742694377899\n",
      "step: 413, cost: 0.051826998591423035\n",
      "step: 414, cost: 0.051705244928598404\n",
      "step: 415, cost: 0.047723811119794846\n",
      "step: 416, cost: 0.05094097927212715\n",
      "step: 417, cost: 0.05756906792521477\n",
      "step: 418, cost: 0.05546236410737038\n",
      "step: 419, cost: 0.05241287872195244\n",
      "step: 420, cost: 0.04771362990140915\n",
      "step: 421, cost: 0.046085014939308167\n",
      "step: 422, cost: 0.055335883051157\n",
      "step: 423, cost: 0.05039912462234497\n",
      "step: 424, cost: 0.05154269561171532\n",
      "step: 425, cost: 0.04649003595113754\n",
      "step: 426, cost: 0.0507279597222805\n",
      "step: 427, cost: 0.05872762203216553\n",
      "step: 428, cost: 0.0576988160610199\n",
      "step: 429, cost: 0.0530204251408577\n",
      "step: 430, cost: 0.05754648894071579\n",
      "step: 431, cost: 0.06161177158355713\n",
      "step: 432, cost: 0.05242081359028816\n",
      "step: 433, cost: 0.05700616165995598\n",
      "step: 434, cost: 0.04532473534345627\n",
      "step: 435, cost: 0.05796409025788307\n",
      "step: 436, cost: 0.05781308561563492\n",
      "step: 437, cost: 0.05391024425625801\n",
      "step: 438, cost: 0.059438761323690414\n",
      "step: 439, cost: 0.04828490689396858\n",
      "step: 440, cost: 0.09645617753267288\n",
      "step: 441, cost: 0.10059623420238495\n",
      "step: 442, cost: 0.10130218416452408\n",
      "step: 443, cost: 0.09344109147787094\n",
      "step: 444, cost: 0.1060156375169754\n",
      "step: 445, cost: 0.09496720135211945\n",
      "step: 446, cost: 0.09440810233354568\n",
      "step: 447, cost: 0.10267352312803268\n",
      "step: 448, cost: 0.09279196709394455\n",
      "step: 449, cost: 0.10201293975114822\n",
      "step: 450, cost: 0.0993112251162529\n",
      "step: 451, cost: 0.09923093765974045\n",
      "step: 452, cost: 0.09873025119304657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 453, cost: 0.09119074791669846\n",
      "step: 454, cost: 0.09651841223239899\n",
      "step: 455, cost: 0.06832385808229446\n",
      "step: 456, cost: 0.08733825385570526\n",
      "step: 457, cost: 0.11669967323541641\n",
      "step: 458, cost: 0.11256285011768341\n",
      "step: 459, cost: 0.09348117560148239\n",
      "step: 460, cost: 0.04552656412124634\n",
      "step: 461, cost: 0.0858067125082016\n",
      "step: 462, cost: 0.07444082200527191\n",
      "step: 463, cost: 0.06945180147886276\n",
      "step: 464, cost: 0.08619771152734756\n",
      "step: 465, cost: 0.062236953526735306\n",
      "step: 466, cost: 0.07181594520807266\n",
      "step: 467, cost: 0.08063165843486786\n",
      "step: 468, cost: 0.06671218574047089\n",
      "step: 469, cost: 0.07547857612371445\n",
      "step: 470, cost: 0.08639656752347946\n",
      "step: 471, cost: 0.054486293345689774\n",
      "step: 472, cost: 0.067299023270607\n",
      "step: 473, cost: 0.10767562687397003\n",
      "step: 474, cost: 0.05677814036607742\n",
      "step: 475, cost: 0.07506375759840012\n",
      "step: 476, cost: 0.04965883865952492\n",
      "step: 477, cost: 0.05007896572351456\n",
      "step: 478, cost: 0.05518937483429909\n",
      "step: 479, cost: 0.05407803878188133\n",
      "step: 480, cost: 0.046776581555604935\n",
      "step: 481, cost: 0.039709534496068954\n",
      "step: 482, cost: 0.05405281111598015\n",
      "step: 483, cost: 0.04496139660477638\n",
      "step: 484, cost: 0.06029229611158371\n",
      "step: 485, cost: 0.06230524182319641\n",
      "step: 486, cost: 0.04483373090624809\n",
      "step: 487, cost: 0.04580880329012871\n",
      "step: 488, cost: 0.04885851591825485\n",
      "step: 489, cost: 0.07033343613147736\n",
      "step: 490, cost: 0.06395301967859268\n",
      "step: 491, cost: 0.049746111035346985\n",
      "step: 492, cost: 0.05317600816488266\n",
      "step: 493, cost: 0.04750312864780426\n",
      "step: 494, cost: 0.05151251703500748\n",
      "step: 495, cost: 0.05444518104195595\n",
      "step: 496, cost: 0.07429176568984985\n",
      "step: 497, cost: 0.059172339737415314\n",
      "step: 498, cost: 0.05215545371174812\n",
      "step: 499, cost: 0.04723861813545227\n",
      "step: 500, cost: 0.05832817032933235\n",
      "step: 501, cost: 0.056629810482263565\n",
      "step: 502, cost: 0.07089550048112869\n",
      "step: 503, cost: 0.05124809592962265\n",
      "step: 504, cost: 0.05429335683584213\n",
      "step: 505, cost: 0.06181950494647026\n",
      "step: 506, cost: 0.054720718413591385\n",
      "step: 507, cost: 0.054647788405418396\n",
      "step: 508, cost: 0.05337975546717644\n",
      "step: 509, cost: 0.05596945434808731\n",
      "step: 510, cost: 0.059801630675792694\n",
      "step: 511, cost: 0.05737854167819023\n",
      "step: 512, cost: 0.056187551468610764\n",
      "step: 513, cost: 0.05250128358602524\n",
      "step: 514, cost: 0.06159724295139313\n",
      "step: 515, cost: 0.05725514516234398\n",
      "step: 516, cost: 0.06419305503368378\n",
      "step: 517, cost: 0.059177882969379425\n",
      "step: 518, cost: 0.05276533588767052\n",
      "step: 519, cost: 0.05435551702976227\n",
      "step: 520, cost: 0.049066461622714996\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter('../temp/graph/graph_summary')\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    print('Learning started.')\n",
    "    for i in range(521):  #Batch number\n",
    "        Batch_size = len(Batch_RE_img_data)\n",
    "        feed_dict = {X_right: Batch_RE_img_data[i],\n",
    "                         X_left: Batch_LE_img_data[i],\n",
    "                         Y:Batch_label[i], keep_prob: 0.7}\n",
    "        s, _ = sess.run([summary, optimizer], feed_dict=feed_dict)\n",
    "        c = sess.run(cost, feed_dict=feed_dict)\n",
    "        writer.add_summary(s, global_step=i)\n",
    "        print('step: {0}, cost: {1}'.format(i, c))\n",
    "        save_path = saver.save(sess,'../temp/variables/model.ckpt') \n",
    "        \n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    predictions = logits.eval(feed_dict = {\n",
    "            X_right: Batch_RE_img_data[39],\n",
    "            X_left: Batch_LE_img_data[39],\n",
    "            keep_prob: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8046875 , 0.86      ],\n",
       "       [0.9375    , 0.585     ],\n",
       "       [0.32578125, 0.62625   ],\n",
       "       ...,\n",
       "       [1.015625  , 0.2725    ],\n",
       "       [0.90703125, 0.625     ],\n",
       "       [0.6109375 , 0.2925    ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Batch_label[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.662403 ,  1.9034401],\n",
       "       [-4.6782975,  1.9079642],\n",
       "       [-4.66164  ,  1.9132535],\n",
       "       ...,\n",
       "       [-4.60349  ,  1.9124138],\n",
       "       [-4.61962  ,  1.9010751],\n",
       "       [-4.592382 ,  1.9093984]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
