{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import matplotlib.cm as cm\n",
    "from scipy.misc import imresize\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import scipy.io as io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extraction(path_left, path_right):\n",
    "    img_path_left = glob(path_left + '/*.jpg')\n",
    "    img_path_right = glob(path_right + '/*.jpg')\n",
    "    img_path_left.sort()\n",
    "    img_path_right.sort()\n",
    "    data_right = []\n",
    "    data_left = []\n",
    "    for i in range(len(img_path_left)):\n",
    "        img_left = cv2.imread(img_path_left[i], 0)\n",
    "        img_right = cv2.imread(img_path_right[i], 0)\n",
    "        data_left.append(img_left)\n",
    "        data_right.append(img_right)\n",
    "    return [data_right, data_left]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_extraction(path, screen_path):\n",
    "    txt = pd.read_csv(path, sep = ' ', header= None)\n",
    "    screen = io.loadmat(screen_path)\n",
    "    df_anno = pd.DataFrame(txt)\n",
    "    data_frame = pd.DataFrame()\n",
    "    data_frame['Screen_x'] = df_anno[:][24] / screen['width_pixel'][0][0]\n",
    "    data_frame['Screen_y'] = df_anno[:][25] / screen['height_pixel'][0][0]\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_right = glob('/home/spow12/Codes/Data/MPIIGaze/Data/Our_data_right/*/*/')\n",
    "image_path_left = glob('/home/spow12/Codes/Data/MPIIGaze/Data/Our_data_left/*/*/')\n",
    "image_path_left.sort()\n",
    "image_path_right.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_optimization(day_list, path):\n",
    "    all_path = []\n",
    "    cnt = 0\n",
    "    for i in range(len(day_list)): # 15번 \n",
    "        temp = []\n",
    "        for j in range(day_list[i]):\n",
    "            temp.append(path[cnt])\n",
    "            cnt += 1\n",
    "        all_path.append(temp)\n",
    "    return all_path\n",
    "\n",
    "day_list = [39, 69, 39, 65, 25, 38, 62, 56, 47, 20, 16, 19, 7, 12, 7]  \n",
    "image_path_left_op = path_optimization(day_list, image_path_left)\n",
    "image_path_right_op = path_optimization(day_list, image_path_right)\n",
    "\n",
    "label_path = glob('/home/spow12/Codes/Data/MPIIGaze/Data/Original/*/*/annotation.txt') # 521개\n",
    "label_path.sort()\n",
    "label_path = path_optimization(day_list, label_path)\n",
    "#label_path = path_optimzation(glob('C:/MPIIGaze/MPIIGaze/Data/Original/*/*/annotation.txt'))\n",
    "screen_size_path = glob('/home/spow12/Codes/Data/MPIIGaze/Data/Original/*/Calibration/screenSize.mat') # 15개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15): # 총 15명\n",
    "    data = [] # 사람별로 나눈 데이터\n",
    "    label = [] # 사람별로 나눈 라벨   \n",
    "    for j in range(day_list[i]): # 각 사람에 맞는 day만큼\n",
    "        data.append(data_extraction(image_path_left_op[i][j], image_path_right_op[i][j]))\n",
    "        label.append(label_extraction(label_path[i][j], screen_size_path[i]))\n",
    "    exec('p%d_data = data' % (i))\n",
    "    exec('p%d_label = label' % (i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_img_label(image_shape, label_shape, data_list, label_list):\n",
    "    length = len(data_list[0])\n",
    "    left_img = np.zeros([length,image_shape])\n",
    "    right_img = np.zeros([length,image_shape])\n",
    "    label = np.zeros([length,label_shape])\n",
    "    for n in range(length):\n",
    "        left_img[n, :] = data_list[1][n].reshape(image_shape)      \n",
    "        right_img[n, :] = data_list[0][n].reshape(image_shape)\n",
    "        label[n, :] = [label_list['Screen_x'][n], label_list['Screen_y'][n]]\n",
    "        \n",
    "    return right_img, left_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_RE_img_data = []\n",
    "Batch_LE_img_data = []\n",
    "#Batch_LE_gaze_data = []\n",
    "#Batch_RE_gaze_data = [] \n",
    "#Batch_LE_pose_data = []\n",
    "#Batch_RE_pose_data = []\n",
    "Batch_label = []\n",
    "for i in range(15):\n",
    "    for j in range(day_list[i]): #days list in Data  po -> 39days  ..\n",
    "        exec('RE_img_data, LE_img_data, dataset_label = make_dataset_img_label(36 * 60, 2, p{0}_data[{1}], p{2}_label[{3}])'\n",
    "             .format(i, j, i, j))\n",
    "        Batch_RE_img_data.append(RE_img_data)\n",
    "        Batch_LE_img_data.append(LE_img_data)\n",
    "#         Batch_LE_gaze_data.append(LE_gaze_data)\n",
    "#         Batch_RE_gaze_data.append(RE_gaze_data)\n",
    "#         Batch_LE_pose_data.append(LE_pose_data)\n",
    "#         Batch_RE_pose_data.append(RE_pose_data)\n",
    "        Batch_label.append(dataset_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make graph for eye images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_for_image(param):\n",
    "    X_img, keep_prob, name, my_batch_norm_layer = param[0], param[1], param[2], param[3]\n",
    "    with tf.name_scope(\"Image_layer_1_\" + name) as scope:\n",
    "        W1 = tf.Variable(tf.random_normal([4, 4, 1, 20], stddev=0.01))\n",
    "        L1 = tf.nn.conv2d(X_img, W1, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        bn1 = my_batch_norm_layer(L1)\n",
    "        bn1_act = tf.nn.elu(bn1)\n",
    "        \n",
    "    with tf.name_scope(\"Image_layer_2_\" + name) as scope:\n",
    "        W2 = tf.Variable(tf.random_normal([2, 2, 20, 40], stddev=0.01))\n",
    "        L2 = tf.nn.conv2d(bn1_act, W2, strides=[1, 2, 2, 1], padding='SAME')\n",
    "        bn2 = my_batch_norm_layer(L2)\n",
    "        bn2_act = tf.nn.elu(bn2)\n",
    "        L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "        L2_flat = tf.reshape(L2, [-1, 5 * 8 * 40])\n",
    "    \n",
    "    with tf.name_scope(\"Image_layer_3_\" + name) as scope:\n",
    "        W3 = tf.get_variable(\"W3_\"+name ,shape= [400, 256],\n",
    "                             initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        b3 = tf.Variable(tf.random_normal([256]))\n",
    "        L3 = tf.nn.relu(tf.matmul(L2_flat, W3) + b3)\n",
    "        L3 = tf.nn.dropout(L3, keep_prob= keep_prob)\n",
    "\n",
    "    with tf.name_scope(\"Image_layer_4_\" + name) as scope:\n",
    "        W4 = tf.get_variable(\"W4_\"+name ,shape= [256, 128],\n",
    "                             initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        b4 = tf.Variable(tf.random_normal([128]))\n",
    "        L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "        L4 = tf.nn.dropout(L4, keep_prob= keep_prob)\n",
    "\n",
    "    with tf.name_scope(\"Image_layer_5_\" + name) as scope:\n",
    "        W5 = tf.get_variable(\"W5_\"+name ,shape= [128, 64],\n",
    "                             initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        b5 = tf.Variable(tf.random_normal([64]))\n",
    "        logit = tf.matmul(L4, W5) + b5\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = keras.Sequential()\n",
    "temp_2 = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_for_image_2(param, model = model_graph):\n",
    "    X_img, keep_prob, name = param[0], param[1], param[2]\n",
    "    with model_graph.as_default():\n",
    "        with tf.name_scope(name +'_image_layers')\n",
    "            conv_1 = Conv2D(20, [7, 7], strides = [1, 1],\n",
    "                   activation = 'elu', padding='same', name= 'conv_1_'+name)(X_img)\n",
    "            bn_1 = BatchNormalization(name= 'bn_1_'+name)(conv_1)\n",
    "            \n",
    "            conv_2 = Conv2D(20, [2, 2], strides = [2, 2],\n",
    "                    activation = 'elu', padding='same', name= 'conv_2'+name)(bn_1)\n",
    "            bn_2 = BatchNormalization(name= 'bn_2_'+name)(conv_2)\n",
    "            \n",
    "            conv_3 = Conv2D(40, [5, 5], strides = [1, 1],\n",
    "                   activation = 'elu', padding='valid',name= 'conv_3'+name)(bn_2)\n",
    "            bn_3 = BatchNormalization(name= 'bn_3_'+name)(conv_3)\n",
    "            \n",
    "            conv_4 =Conv2D(40, [2, 2], strides = [2, 2],\n",
    "                    activation = 'elu', padding='same', name = 'conv_4'+name)(bn_3)\n",
    "            bn_4 = BatchNormalization(name= 'bn_4_'+name)(conv_4)\n",
    "            \n",
    "            conv_5 = Conv2D(80, [6, 6], strides = [1, 1],\n",
    "                            activation = 'elu', padding='valid')\n",
    "            bn_5 = BatchNormalization(name= 'bn_4_'+name)(conv_5)\n",
    "            Dense(units= 280, input_dim = 560, activation= 'elu')\n",
    "            Dropout(keep_prob)\n",
    "            Dense(units= 100, input_dim = 280, activation= 'elu')\n",
    "    \n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate the Image layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(LE_param, RE_param):\n",
    "    \n",
    "    LE = model_for_image(LE_param)\n",
    "    RE = model_for_image(RE_param)\n",
    "    with tf.name_scope(\"Layer_Image_merged_1\") as scope:\n",
    "        LE_W_img = tf.get_variable(\"LE_W_img\" ,shape= [64, 32],\n",
    "                                     initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        RE_W_img = tf.get_variable(\"RE_W_img\" ,shape= [64, 32],\n",
    "                                     initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        merged_img_b = tf.Variable(tf.random_normal([32]))\n",
    "        layer_img = tf.matmul(LE, LE_W_img) + tf.matmul(RE, RE_W_img) + merged_img_b\n",
    "        layer_img = tf.nn.dropout(layer_img, keep_prob= keep_prob)\n",
    "\n",
    "    with tf.name_scope(\"Layer_Image_merged_2\") as scope:\n",
    "        img_W = tf.get_variable(\"img_W\" ,shape= [32, 16],\n",
    "                                     initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        img_b = tf.Variable(tf.random_normal([16]))\n",
    "        layer2_img = tf.matmul(layer_img, img_W) + img_b\n",
    "        layer2_img = tf.nn.dropout(layer2_img, keep_prob= keep_prob)\n",
    "\n",
    "    with tf.name_scope(\"Layer_Image_merged_3\") as scope:\n",
    "        img_W2 = tf.get_variable(\"img_W2\" ,shape= [16, 8],\n",
    "                                     initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        img_b2 = tf.Variable(tf.random_normal([8]))\n",
    "        layer3_img= tf.matmul(layer2_img, img_W2) + img_b2\n",
    "        layer3_img = tf.nn.dropout(layer3_img, keep_prob= keep_prob)\n",
    "\n",
    "    with tf.name_scope(\"Layer_Image_merged_Last\") as scope:\n",
    "        Final_W_img = tf.get_variable(\"Final_W_img\" ,shape= [8, 2],\n",
    "                                     initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        logits =  tf.matmul(layer3_img, Final_W_img)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope(\"Layer_Image_merged_1\") as scope:\n",
    "#     LE_W_img = tf.get_variable(\"LE_W_img\" ,shape= [64, 32],\n",
    "#                              initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "#     RE_W_img = tf.get_variable(\"RE_W_img\" ,shape= [64, 32],\n",
    "#                              initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "    \n",
    "#     merged_img_b = tf.Variable(tf.random_normal([32]))\n",
    "#     layer_img = tf.matmul(LE_img, LE_W_img) + tf.matmul(RE_img, RE_W_img) + merged_img_b\n",
    "#     layer_img = tf.nn.dropout(layer_img, keep_prob= keep_prob)\n",
    "#     bn_merged = batch_norm_layer(layer_img)\n",
    "#     bn_merged_act = tf.nn.elu(bn_merged)\n",
    "    \n",
    "# with tf.name_scope(\"Layer_Image_merged_2\") as scope:\n",
    "#     img_W = tf.get_variable(\"img_W\" ,shape= [32, 16],\n",
    "#                              initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "#     img_b = tf.Variable(tf.random_normal([16]))\n",
    "#     layer2_img = tf.matmul(bn_merged_act, img_W) + img_b\n",
    "#     layer2_img = tf.nn.dropout(layer2_img, keep_prob= keep_prob)\n",
    "#     bn_merged2 = batch_norm_layer(layer2_img)\n",
    "#     bn_merged2_act = tf.nn.elu(bn_merged2)\n",
    "    \n",
    "# with tf.name_scope(\"Layer_Image_merged_3\") as scope:\n",
    "#     img_W2 = tf.get_variable(\"img_W2\" ,shape= [16, 8],\n",
    "#                              initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "#     img_b2 = tf.Variable(tf.random_normal([8]))\n",
    "#     layer3_img= tf.matmul(bn_merged2_act, img_W2) + img_b2\n",
    "#     layer3_img = tf.nn.dropout(layer3_img, keep_prob= keep_prob)\n",
    "#     bn_merged3 = batch_norm_layer(layer3_img)\n",
    "#     bn_merged3_act = tf.nn.elu(bn_merged3)\n",
    "\n",
    "# with tf.name_scope(\"Layer_Image_merged_Last\") as scope:\n",
    "# #     Final_W_img = tf.get_variable(\"Final_W_img\" ,shape= [8, 3],\n",
    "# #                              initializer= tf.contrib.layers.xavier_initializer())\n",
    "# #     Final_b_img = tf.Variable(tf.random_normal([3]))\n",
    "# #     Final_layer_img= tf.nn.relu(tf.matmul(layer3_img, Final_W_img) + Final_b_img)\n",
    "#     Final_W_img = tf.get_variable(\"Final_W_img\" ,shape= [8, 2],\n",
    "#                              initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "#     logits =  tf.matmul(layer3_img, Final_W_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variable and make graphs using function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#hyper parameter\n",
    "training = tf.placeholder_with_default(False, shape=[], name=\"training\")\n",
    "batch_norm_momentum = 0.9\n",
    "learning_rate = 0.01\n",
    "Y = tf.placeholder(tf.float32, [None, 2])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# define input\n",
    "X_left = tf.placeholder(tf.float32, [None, 2160])\n",
    "X_right = tf.placeholder(tf.float32, [None, 2160])\n",
    "X_img_left = tf.reshape(X_left, [-1, 36, 60, 1])   \n",
    "X_img_right = tf.reshape(X_right, [-1, 36, 60, 1]) \n",
    "\n",
    "param_left = (X_img_left, keep_prob, 'left_eye')\n",
    "param_right = (X_img_right, keep_prob, 'right_eye')\n",
    "\n",
    "CHECK_POINT_DIR = TB_SUMMARY_DIR = '../temp/graph/'\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0916 18:49:27.314495 140019386353408 deprecation.py:506] From <ipython-input-26-695681a8c7b2>:7: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0916 18:49:28.441243 140019386353408 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_graph = model(LE_param= param_left, RE_param= param_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"cost\"):\n",
    "    cost = tf.reduce_mean(tf.square(model_graph-Y))\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "last_epoch = tf.Variable(0, name='last_epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find old network weights\n",
      "Learning started\n",
      "Batch_Number:0, Batch_cost: 108.53499603271484\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:1, Batch_cost: 103.316650390625\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:2, Batch_cost: 107.00682830810547\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:3, Batch_cost: 108.6179428100586\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:4, Batch_cost: 108.07077026367188\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:5, Batch_cost: 109.67855072021484\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:6, Batch_cost: 121.95320892333984\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:7, Batch_cost: 108.9684066772461\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:8, Batch_cost: 109.95758056640625\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:9, Batch_cost: 112.97023010253906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:10, Batch_cost: 105.72859191894531\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:11, Batch_cost: 102.42217254638672\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:12, Batch_cost: 109.37923431396484\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:13, Batch_cost: 126.91043090820312\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:14, Batch_cost: 105.19489288330078\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:15, Batch_cost: 104.26631927490234\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:16, Batch_cost: 111.10291290283203\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:17, Batch_cost: 105.90384674072266\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:18, Batch_cost: 115.92139434814453\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:19, Batch_cost: 112.7083740234375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:20, Batch_cost: 112.7518081665039\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:21, Batch_cost: 107.57695007324219\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:22, Batch_cost: 102.64360046386719\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:23, Batch_cost: 106.64378356933594\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:24, Batch_cost: 113.82913970947266\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:25, Batch_cost: 103.51494598388672\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:26, Batch_cost: 107.67794799804688\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:27, Batch_cost: 103.5041732788086\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:28, Batch_cost: 109.85724639892578\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:29, Batch_cost: 106.92716979980469\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:30, Batch_cost: 101.90908813476562\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:31, Batch_cost: 104.00841522216797\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:32, Batch_cost: 109.0464859008789\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:33, Batch_cost: 113.93347930908203\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:34, Batch_cost: 110.78233337402344\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:35, Batch_cost: 105.6002426147461\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:36, Batch_cost: 108.26298522949219\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:37, Batch_cost: 102.72413635253906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:38, Batch_cost: 105.93539428710938\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:39, Batch_cost: 108.05109405517578\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:40, Batch_cost: 125.58958435058594\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:41, Batch_cost: 106.8505630493164\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:42, Batch_cost: 108.49024963378906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:43, Batch_cost: 109.54375457763672\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:44, Batch_cost: 105.78470611572266\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:45, Batch_cost: 112.177001953125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:46, Batch_cost: 110.00106811523438\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:47, Batch_cost: 84.1278305053711\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:48, Batch_cost: 115.5830078125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:49, Batch_cost: 92.37094116210938\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:50, Batch_cost: 109.34373474121094\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:51, Batch_cost: 108.21185302734375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:52, Batch_cost: 112.39344024658203\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:53, Batch_cost: 106.33890533447266\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:54, Batch_cost: 120.26801300048828\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:55, Batch_cost: 109.34648895263672\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:56, Batch_cost: 96.41099548339844\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:57, Batch_cost: 95.69171905517578\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:58, Batch_cost: 106.8824691772461\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:59, Batch_cost: 110.27582550048828\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:60, Batch_cost: 106.28668975830078\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:61, Batch_cost: 112.5287094116211\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:62, Batch_cost: 167.8838348388672\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:63, Batch_cost: 105.05107879638672\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:64, Batch_cost: 107.27359771728516\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:65, Batch_cost: 116.12895202636719\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:66, Batch_cost: 108.82051086425781\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:67, Batch_cost: 110.5906753540039\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:68, Batch_cost: 111.75564575195312\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:69, Batch_cost: 130.3818817138672\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:70, Batch_cost: 114.06584930419922\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:71, Batch_cost: 97.86036682128906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:72, Batch_cost: 90.71737670898438\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:73, Batch_cost: 100.8514404296875\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:74, Batch_cost: 110.66023254394531\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:75, Batch_cost: 104.57546997070312\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:76, Batch_cost: 102.68727111816406\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:77, Batch_cost: 35.53218078613281\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:78, Batch_cost: 81.28697204589844\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:79, Batch_cost: 116.16751098632812\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:80, Batch_cost: 106.1862564086914\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:81, Batch_cost: 102.51239776611328\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:82, Batch_cost: 94.0624008178711\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:83, Batch_cost: 111.7058334350586\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:84, Batch_cost: 83.30103302001953\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:85, Batch_cost: 116.65483093261719\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:86, Batch_cost: 125.89240264892578\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:87, Batch_cost: 121.17070007324219\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:88, Batch_cost: 102.83457946777344\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:89, Batch_cost: 100.4893798828125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:90, Batch_cost: 110.4039077758789\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:91, Batch_cost: 122.30074310302734\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:92, Batch_cost: 112.07176208496094\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:93, Batch_cost: 102.75182342529297\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:94, Batch_cost: 114.00812530517578\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:95, Batch_cost: 112.39916229248047\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:96, Batch_cost: 109.88780975341797\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:97, Batch_cost: 106.7730484008789\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:98, Batch_cost: 118.18546295166016\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:99, Batch_cost: 107.36579132080078\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:100, Batch_cost: 109.5453872680664\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:101, Batch_cost: 110.68018341064453\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:102, Batch_cost: 96.14476013183594\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:103, Batch_cost: 95.61951446533203\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:104, Batch_cost: 109.5009765625\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:105, Batch_cost: 95.1241455078125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:106, Batch_cost: 105.58443450927734\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:107, Batch_cost: 10.960075378417969\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:108, Batch_cost: 103.96855163574219\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:109, Batch_cost: 106.56538391113281\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:110, Batch_cost: 104.939208984375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:111, Batch_cost: 104.06712341308594\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:112, Batch_cost: 102.74463653564453\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:113, Batch_cost: 117.26591491699219\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:114, Batch_cost: 103.67149353027344\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:115, Batch_cost: 116.61138916015625\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:116, Batch_cost: 101.88204956054688\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:117, Batch_cost: 102.70911407470703\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:118, Batch_cost: 107.53516387939453\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:119, Batch_cost: 111.2594985961914\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:120, Batch_cost: 110.28978729248047\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:121, Batch_cost: 106.17849731445312\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:122, Batch_cost: 107.86259460449219\n",
      "Saving network...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_Number:123, Batch_cost: 113.82056427001953\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:124, Batch_cost: 123.22209167480469\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:125, Batch_cost: 117.86693572998047\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:126, Batch_cost: 107.95208740234375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:127, Batch_cost: 117.37590026855469\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:128, Batch_cost: 108.5124282836914\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:129, Batch_cost: 105.24198913574219\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:130, Batch_cost: 108.68244934082031\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:131, Batch_cost: 102.81598663330078\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:132, Batch_cost: 108.40370178222656\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:133, Batch_cost: 108.09427642822266\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:134, Batch_cost: 105.42884063720703\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:135, Batch_cost: 95.89186096191406\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:136, Batch_cost: 110.82381439208984\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:137, Batch_cost: 111.45832061767578\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:138, Batch_cost: 103.09212493896484\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:139, Batch_cost: 104.84187316894531\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:140, Batch_cost: 117.84024810791016\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:141, Batch_cost: 108.41543579101562\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:142, Batch_cost: 118.20516967773438\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:143, Batch_cost: 108.23146057128906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:144, Batch_cost: 119.14884948730469\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:145, Batch_cost: 108.42861938476562\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:146, Batch_cost: 131.17025756835938\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:147, Batch_cost: 107.1606216430664\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:148, Batch_cost: 117.81061553955078\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:149, Batch_cost: 108.30298614501953\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:150, Batch_cost: 117.4797134399414\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:151, Batch_cost: 124.57886505126953\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:152, Batch_cost: 131.71434020996094\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:153, Batch_cost: 105.0360107421875\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:154, Batch_cost: 118.1812744140625\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:155, Batch_cost: 105.75071716308594\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:156, Batch_cost: 101.15787506103516\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:157, Batch_cost: 95.46263885498047\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:158, Batch_cost: 108.8471908569336\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:159, Batch_cost: 103.95193481445312\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:160, Batch_cost: 103.38384246826172\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:161, Batch_cost: 119.93916320800781\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:162, Batch_cost: 103.7913818359375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:163, Batch_cost: 115.94290161132812\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:164, Batch_cost: 99.66951751708984\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:165, Batch_cost: 102.71913146972656\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:166, Batch_cost: 108.90447235107422\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:167, Batch_cost: 109.54097747802734\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:168, Batch_cost: 110.19172668457031\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:169, Batch_cost: 133.76222229003906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:170, Batch_cost: 107.03761291503906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:171, Batch_cost: 113.76268005371094\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:172, Batch_cost: 113.31556701660156\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:173, Batch_cost: 112.49629211425781\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:174, Batch_cost: 100.5978012084961\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:175, Batch_cost: 104.0559310913086\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:176, Batch_cost: 99.85743713378906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:177, Batch_cost: 112.97118377685547\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:178, Batch_cost: 111.57060241699219\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:179, Batch_cost: 118.63257598876953\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:180, Batch_cost: 135.707275390625\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:181, Batch_cost: 116.36734771728516\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:182, Batch_cost: 110.28427124023438\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:183, Batch_cost: 110.56460571289062\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:184, Batch_cost: 110.2927474975586\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:185, Batch_cost: 110.44586181640625\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:186, Batch_cost: 122.64082336425781\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:187, Batch_cost: 113.32078552246094\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:188, Batch_cost: 105.04808044433594\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:189, Batch_cost: 96.59507751464844\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:190, Batch_cost: 106.97838592529297\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:191, Batch_cost: 112.71343994140625\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:192, Batch_cost: 101.13262939453125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:193, Batch_cost: 113.8244857788086\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:194, Batch_cost: 105.50894927978516\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:195, Batch_cost: 103.08712005615234\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:196, Batch_cost: 111.12812805175781\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:197, Batch_cost: 113.11359405517578\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:198, Batch_cost: 99.8892822265625\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:199, Batch_cost: 103.46161651611328\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:200, Batch_cost: 108.3063735961914\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:201, Batch_cost: 112.82612609863281\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:202, Batch_cost: 107.47502136230469\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:203, Batch_cost: 103.8794174194336\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:204, Batch_cost: 95.89619445800781\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:205, Batch_cost: 115.85736083984375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:206, Batch_cost: 110.11156463623047\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:207, Batch_cost: 107.96813201904297\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:208, Batch_cost: 109.03604125976562\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:209, Batch_cost: 117.39960479736328\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:210, Batch_cost: 110.18523406982422\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:211, Batch_cost: 116.850830078125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:212, Batch_cost: 109.78502655029297\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:213, Batch_cost: 119.56592559814453\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:214, Batch_cost: 111.48979949951172\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:215, Batch_cost: 110.51827239990234\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:216, Batch_cost: 86.44219970703125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:217, Batch_cost: 102.29605102539062\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:218, Batch_cost: 101.64009094238281\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:219, Batch_cost: 105.62030029296875\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:220, Batch_cost: 98.50657653808594\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:221, Batch_cost: 108.27750396728516\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:222, Batch_cost: 119.55853271484375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:223, Batch_cost: 97.69339752197266\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:224, Batch_cost: 123.62247467041016\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:225, Batch_cost: 110.69536590576172\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:226, Batch_cost: 103.61714172363281\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:227, Batch_cost: 113.82244873046875\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:228, Batch_cost: 112.6174545288086\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:229, Batch_cost: 119.0806655883789\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:230, Batch_cost: 103.15613555908203\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:231, Batch_cost: 107.59815216064453\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:232, Batch_cost: 76.82354736328125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:233, Batch_cost: 96.51438903808594\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:234, Batch_cost: 117.49186706542969\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:235, Batch_cost: 109.59577941894531\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:236, Batch_cost: 104.39871978759766\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:237, Batch_cost: 117.56306457519531\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:238, Batch_cost: 106.52932739257812\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:239, Batch_cost: 108.7110595703125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:240, Batch_cost: 133.1768341064453\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:241, Batch_cost: 133.46234130859375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:242, Batch_cost: 111.26848602294922\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:243, Batch_cost: 115.8554916381836\n",
      "Saving network...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_Number:244, Batch_cost: 112.02115631103516\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:245, Batch_cost: 107.68020629882812\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:246, Batch_cost: 102.70780944824219\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:247, Batch_cost: 110.61947631835938\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:248, Batch_cost: 108.7024917602539\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:249, Batch_cost: 100.4928970336914\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:250, Batch_cost: 106.14307403564453\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:251, Batch_cost: 112.53583526611328\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:252, Batch_cost: 106.94392395019531\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:253, Batch_cost: 106.21456146240234\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:254, Batch_cost: 101.74836730957031\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:255, Batch_cost: 112.34025573730469\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:256, Batch_cost: 122.74515533447266\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:257, Batch_cost: 107.09293365478516\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:258, Batch_cost: 110.96163940429688\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:259, Batch_cost: 104.2616958618164\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:260, Batch_cost: 98.79845428466797\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:261, Batch_cost: 108.30064392089844\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:262, Batch_cost: 110.21975708007812\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:263, Batch_cost: 99.8278579711914\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:264, Batch_cost: 118.37577819824219\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:265, Batch_cost: 113.37035369873047\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:266, Batch_cost: 163.4596710205078\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:267, Batch_cost: 97.56491088867188\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:268, Batch_cost: 104.21409606933594\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:269, Batch_cost: 106.03894805908203\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:270, Batch_cost: 138.67698669433594\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:271, Batch_cost: 86.2701644897461\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:272, Batch_cost: 114.06275177001953\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:273, Batch_cost: 94.29774475097656\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:274, Batch_cost: 93.56439971923828\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:275, Batch_cost: 101.0076675415039\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:276, Batch_cost: 106.27149963378906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:277, Batch_cost: 90.05287170410156\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:278, Batch_cost: 106.55888366699219\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:279, Batch_cost: 103.8747787475586\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:280, Batch_cost: 113.92977142333984\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:281, Batch_cost: 118.4344711303711\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:282, Batch_cost: 93.22176361083984\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:283, Batch_cost: 107.26421356201172\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:284, Batch_cost: 127.15367889404297\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:285, Batch_cost: 109.22541809082031\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:286, Batch_cost: 109.1565170288086\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:287, Batch_cost: 113.39144134521484\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:288, Batch_cost: 116.87973022460938\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:289, Batch_cost: 105.19525146484375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:290, Batch_cost: 121.247314453125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:291, Batch_cost: 100.18682861328125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:292, Batch_cost: 102.54474639892578\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:293, Batch_cost: 102.08287811279297\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:294, Batch_cost: 98.71207427978516\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:295, Batch_cost: 93.01515197753906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:296, Batch_cost: 99.00410461425781\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:297, Batch_cost: 91.20344543457031\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:298, Batch_cost: 104.39335632324219\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:299, Batch_cost: 113.11038970947266\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:300, Batch_cost: 104.38333129882812\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:301, Batch_cost: 112.852783203125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:302, Batch_cost: 74.32014465332031\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:303, Batch_cost: 101.76905822753906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:304, Batch_cost: 80.6214828491211\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:305, Batch_cost: 125.8423080444336\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:306, Batch_cost: 105.4769287109375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:307, Batch_cost: 108.73750305175781\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:308, Batch_cost: 99.81770324707031\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:309, Batch_cost: 120.84523010253906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:310, Batch_cost: 108.5801773071289\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:311, Batch_cost: 121.98286437988281\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:312, Batch_cost: 114.73290252685547\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:313, Batch_cost: 109.61925506591797\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:314, Batch_cost: 100.32947540283203\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:315, Batch_cost: 119.7597885131836\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:316, Batch_cost: 86.31494903564453\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:317, Batch_cost: 123.07532501220703\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:318, Batch_cost: 132.05865478515625\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:319, Batch_cost: 118.92951202392578\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:320, Batch_cost: 120.66893768310547\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:321, Batch_cost: 103.14607238769531\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:322, Batch_cost: 113.32801055908203\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:323, Batch_cost: 91.24864959716797\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:324, Batch_cost: 101.43070220947266\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:325, Batch_cost: 111.72389221191406\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:326, Batch_cost: 103.94330596923828\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:327, Batch_cost: 113.28030395507812\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:328, Batch_cost: 92.87222290039062\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:329, Batch_cost: 165.66098022460938\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:330, Batch_cost: 94.73955535888672\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:331, Batch_cost: 105.96492767333984\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:332, Batch_cost: 85.78946685791016\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:333, Batch_cost: 103.36371612548828\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:334, Batch_cost: 93.05847930908203\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:335, Batch_cost: 102.9643325805664\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:336, Batch_cost: 91.97411346435547\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:337, Batch_cost: 131.7747802734375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:338, Batch_cost: 117.7470474243164\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:339, Batch_cost: 125.21583557128906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:340, Batch_cost: 103.31920623779297\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:341, Batch_cost: 115.203125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:342, Batch_cost: 114.02783203125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:343, Batch_cost: 121.5249252319336\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:344, Batch_cost: 88.85923767089844\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:345, Batch_cost: 106.153564453125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:346, Batch_cost: 116.18359375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:347, Batch_cost: 87.24363708496094\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:348, Batch_cost: 103.6335220336914\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:349, Batch_cost: 112.44132232666016\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:350, Batch_cost: 96.11964416503906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:351, Batch_cost: 97.39057159423828\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:352, Batch_cost: 120.98543548583984\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:353, Batch_cost: 120.64632415771484\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:354, Batch_cost: 111.33210754394531\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:355, Batch_cost: 99.56963348388672\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:356, Batch_cost: 95.39690399169922\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:357, Batch_cost: 112.91344451904297\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:358, Batch_cost: 132.8788299560547\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:359, Batch_cost: 106.21621704101562\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:360, Batch_cost: 104.77481842041016\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:361, Batch_cost: 96.42953491210938\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:362, Batch_cost: 130.1215057373047\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:363, Batch_cost: 109.08279418945312\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:364, Batch_cost: 99.50721740722656\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:365, Batch_cost: 103.57862854003906\n",
      "Saving network...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_Number:366, Batch_cost: 108.95014953613281\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:367, Batch_cost: 97.29716491699219\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:368, Batch_cost: 115.97102355957031\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:369, Batch_cost: 107.92047119140625\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:370, Batch_cost: 126.73409271240234\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:371, Batch_cost: 88.37869262695312\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:372, Batch_cost: 99.69733428955078\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:373, Batch_cost: 100.9037094116211\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:374, Batch_cost: 109.65325164794922\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:375, Batch_cost: 109.55270385742188\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:376, Batch_cost: 99.82965850830078\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:377, Batch_cost: 90.49108123779297\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:378, Batch_cost: 111.15238952636719\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:379, Batch_cost: 111.10658264160156\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:380, Batch_cost: 116.61859893798828\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:381, Batch_cost: 99.04551696777344\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:382, Batch_cost: 92.5069351196289\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:383, Batch_cost: 107.781982421875\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:384, Batch_cost: 114.13623046875\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:385, Batch_cost: 111.5762939453125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:386, Batch_cost: 123.1564712524414\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:387, Batch_cost: 116.62323760986328\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:388, Batch_cost: 112.51106262207031\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:389, Batch_cost: 137.1763153076172\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:390, Batch_cost: 116.34615325927734\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:391, Batch_cost: 95.73098754882812\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:392, Batch_cost: 113.31062316894531\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:393, Batch_cost: 67.51480865478516\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:394, Batch_cost: 101.62493896484375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:395, Batch_cost: 98.60086059570312\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:396, Batch_cost: 95.9582290649414\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:397, Batch_cost: 102.62318420410156\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:398, Batch_cost: 129.23680114746094\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:399, Batch_cost: 109.3660659790039\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:400, Batch_cost: 84.999267578125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:401, Batch_cost: 82.84883880615234\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:402, Batch_cost: 89.53316497802734\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:403, Batch_cost: 72.29067993164062\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:404, Batch_cost: 83.96524810791016\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:405, Batch_cost: 122.3841323852539\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:406, Batch_cost: 97.35358428955078\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:407, Batch_cost: 90.71827697753906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:408, Batch_cost: 124.62710571289062\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:409, Batch_cost: 100.8155517578125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:410, Batch_cost: 115.88848876953125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:411, Batch_cost: 97.10590362548828\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:412, Batch_cost: 108.88956451416016\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:413, Batch_cost: 113.99396514892578\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:414, Batch_cost: 105.43423461914062\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:415, Batch_cost: 116.76271057128906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:416, Batch_cost: 78.1641616821289\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:417, Batch_cost: 132.1249542236328\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:418, Batch_cost: 143.52487182617188\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:419, Batch_cost: 87.36735534667969\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:420, Batch_cost: 133.68289184570312\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:421, Batch_cost: 91.11104583740234\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:422, Batch_cost: 112.28687286376953\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:423, Batch_cost: 110.58600616455078\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:424, Batch_cost: 165.64695739746094\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:425, Batch_cost: 104.02851867675781\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:426, Batch_cost: 64.62571716308594\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:427, Batch_cost: 115.12489318847656\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:428, Batch_cost: 95.1418228149414\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:429, Batch_cost: 90.2285385131836\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:430, Batch_cost: 115.462158203125\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:431, Batch_cost: 111.75548553466797\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:432, Batch_cost: 115.12897491455078\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:433, Batch_cost: 114.5241928100586\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:434, Batch_cost: 97.36329650878906\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:435, Batch_cost: 97.07817077636719\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:436, Batch_cost: 110.8932113647461\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:437, Batch_cost: 108.95111083984375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:438, Batch_cost: 105.63211059570312\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:439, Batch_cost: 111.8070068359375\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:440, Batch_cost: 111.3336181640625\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:441, Batch_cost: 115.18528747558594\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:442, Batch_cost: 108.63197326660156\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:443, Batch_cost: 110.64307403564453\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:444, Batch_cost: 151.68824768066406\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:445, Batch_cost: 107.9169692993164\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:446, Batch_cost: 104.32072448730469\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:447, Batch_cost: 109.07272338867188\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:448, Batch_cost: 105.77201080322266\n",
      "Saving network...\n",
      "\n",
      "Batch_Number:449, Batch_cost: 113.30708312988281\n",
      "Saving network...\n",
      "\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "cost_graph = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(TB_SUMMARY_DIR)\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoint = tf.train.get_checkpoint_state(CHECK_POINT_DIR)    \n",
    "    \n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        try:\n",
    "            saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "            print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "        except:\n",
    "            print(\"Error on loading old network weights\")\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")\n",
    "\n",
    "    start_from = sess.run(last_epoch)\n",
    "    print('Learning started')\n",
    "    for i in range(start_from, 450):  #Train set size\n",
    "        feed_dict = {X_right: Batch_RE_img_data[i],\n",
    "                    X_left: Batch_LE_img_data[i],\n",
    "                    Y:Batch_label[i], keep_prob: 0.7}\n",
    "        #s, _ = sess.run([summary, optimizer], feed_dict=feed_dict)\n",
    "        #writer.add_summary(s, global_step=global_step)\n",
    "        c= sess.run(cost,feed_dict=feed_dict)\n",
    "        print('Batch_Number:{0}, Batch_cost: {1}'.format(i, c))\n",
    "        save_path = saver.save(sess,'../temp/variables/model.ckpt') \n",
    "        cost_graph.append(c)\n",
    "        global_step += 1\n",
    "        \n",
    "        print(\"Saving network...\\n\")\n",
    "        sess.run(last_epoch.assign(i + 1))\n",
    "        if not os.path.exists(CHECK_POINT_DIR):\n",
    "            os.makedirs(CHECK_POINT_DIR)\n",
    "        saver.save(sess, CHECK_POINT_DIR + \"/model\", global_step=j)\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "import numpy as np    \n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.866622829690313&quot;).pbtxt = 'node {\\n  name: &quot;training/input&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_BOOL\\n        tensor_shape {\\n        }\\n        bool_val: false\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;training&quot;\\n  op: &quot;PlaceholderWithDefault&quot;\\n  input: &quot;training/input&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Placeholder&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 2\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Placeholder_1&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Placeholder_2&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 2160\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Placeholder_3&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 2160\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377$\\\\000\\\\000\\\\000<\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Placeholder_2&quot;\\n  input: &quot;Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape_1/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 4\\n          }\\n        }\\n        tensor_content: &quot;\\\\377\\\\377\\\\377\\\\377$\\\\000\\\\000\\\\000<\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;Placeholder_3&quot;\\n  input: &quot;Reshape_1/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.866622829690313&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cost_graph)\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_data_frame = pd.DataFrame()\n",
    "y_data_frame = pd.DataFrame()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(450, 521):\n",
    "        predictions = model_graph.eval(feed_dict = {\n",
    "                X_right: Batch_RE_img_data[i],\n",
    "                X_left: Batch_LE_img_data[i],\n",
    "                keep_prob: 1})\n",
    "        x_data_frame = pd.concat([pd.DataFrame(predictions), x_data_frame], ignore_index= True)\n",
    "        y_data_frame = pd.concat([pd.DataFrame(Batch_label[i]), y_data_frame], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.290645</td>\n",
       "      <td>-14.006769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.351165</td>\n",
       "      <td>-14.329849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.299284</td>\n",
       "      <td>-13.993162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.306480</td>\n",
       "      <td>-14.053202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.369119</td>\n",
       "      <td>-14.243830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.408422</td>\n",
       "      <td>-14.234203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.286169</td>\n",
       "      <td>-14.148697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.320438</td>\n",
       "      <td>-14.142910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.343582</td>\n",
       "      <td>-14.220028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.244815</td>\n",
       "      <td>-14.076307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.321133</td>\n",
       "      <td>-14.183609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.343460</td>\n",
       "      <td>-14.262471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.353354</td>\n",
       "      <td>-14.025944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.294534</td>\n",
       "      <td>-13.971724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.372996</td>\n",
       "      <td>-14.164814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.356254</td>\n",
       "      <td>-14.065913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.461636</td>\n",
       "      <td>-14.323559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.373918</td>\n",
       "      <td>-13.987829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.362936</td>\n",
       "      <td>-13.969357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.353464</td>\n",
       "      <td>-13.996689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.275583</td>\n",
       "      <td>-14.256238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.365899</td>\n",
       "      <td>-14.090268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.498415</td>\n",
       "      <td>-14.013653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.238300</td>\n",
       "      <td>-14.147352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.302860</td>\n",
       "      <td>-14.017174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.465818</td>\n",
       "      <td>-14.269360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.274925</td>\n",
       "      <td>-14.084674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.406995</td>\n",
       "      <td>-13.985962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.382691</td>\n",
       "      <td>-14.180301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.330436</td>\n",
       "      <td>-14.038247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12319</th>\n",
       "      <td>-1.061528</td>\n",
       "      <td>-13.505957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12320</th>\n",
       "      <td>-1.083420</td>\n",
       "      <td>-13.544744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12321</th>\n",
       "      <td>-1.071946</td>\n",
       "      <td>-13.512463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12322</th>\n",
       "      <td>-1.085612</td>\n",
       "      <td>-13.499512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12323</th>\n",
       "      <td>-1.083938</td>\n",
       "      <td>-13.497822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12324</th>\n",
       "      <td>-1.065640</td>\n",
       "      <td>-13.514324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12325</th>\n",
       "      <td>-1.055166</td>\n",
       "      <td>-13.494580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12326</th>\n",
       "      <td>-1.079917</td>\n",
       "      <td>-13.482646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12327</th>\n",
       "      <td>-1.058770</td>\n",
       "      <td>-13.489414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12328</th>\n",
       "      <td>-1.048842</td>\n",
       "      <td>-13.515354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12329</th>\n",
       "      <td>-1.086797</td>\n",
       "      <td>-13.531395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12330</th>\n",
       "      <td>-1.568627</td>\n",
       "      <td>-14.877522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12331</th>\n",
       "      <td>-1.344709</td>\n",
       "      <td>-14.725714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12332</th>\n",
       "      <td>-1.603326</td>\n",
       "      <td>-15.021224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12333</th>\n",
       "      <td>-1.601675</td>\n",
       "      <td>-14.575672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>-1.595818</td>\n",
       "      <td>-14.426875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>-1.264544</td>\n",
       "      <td>-14.234146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>-1.310516</td>\n",
       "      <td>-14.188839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12337</th>\n",
       "      <td>-1.169034</td>\n",
       "      <td>-14.199857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>-1.278854</td>\n",
       "      <td>-14.497911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12339</th>\n",
       "      <td>-1.279956</td>\n",
       "      <td>-14.499651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12340</th>\n",
       "      <td>-1.178506</td>\n",
       "      <td>-14.005718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12341</th>\n",
       "      <td>-1.190674</td>\n",
       "      <td>-14.492249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12342</th>\n",
       "      <td>-1.259125</td>\n",
       "      <td>-14.465416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12343</th>\n",
       "      <td>-1.256915</td>\n",
       "      <td>-14.308807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12344</th>\n",
       "      <td>-1.167905</td>\n",
       "      <td>-14.399468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12345</th>\n",
       "      <td>-1.327071</td>\n",
       "      <td>-14.395011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12346</th>\n",
       "      <td>-1.277843</td>\n",
       "      <td>-14.402060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12347</th>\n",
       "      <td>-1.312950</td>\n",
       "      <td>-14.710134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12348</th>\n",
       "      <td>-1.235074</td>\n",
       "      <td>-14.603294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12349 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1\n",
       "0     -1.290645 -14.006769\n",
       "1     -1.351165 -14.329849\n",
       "2     -1.299284 -13.993162\n",
       "3     -1.306480 -14.053202\n",
       "4     -1.369119 -14.243830\n",
       "5     -1.408422 -14.234203\n",
       "6     -1.286169 -14.148697\n",
       "7     -1.320438 -14.142910\n",
       "8     -1.343582 -14.220028\n",
       "9     -1.244815 -14.076307\n",
       "10    -1.321133 -14.183609\n",
       "11    -1.343460 -14.262471\n",
       "12    -1.353354 -14.025944\n",
       "13    -1.294534 -13.971724\n",
       "14    -1.372996 -14.164814\n",
       "15    -1.356254 -14.065913\n",
       "16    -1.461636 -14.323559\n",
       "17    -1.373918 -13.987829\n",
       "18    -1.362936 -13.969357\n",
       "19    -1.353464 -13.996689\n",
       "20    -1.275583 -14.256238\n",
       "21    -1.365899 -14.090268\n",
       "22    -1.498415 -14.013653\n",
       "23    -1.238300 -14.147352\n",
       "24    -1.302860 -14.017174\n",
       "25    -1.465818 -14.269360\n",
       "26    -1.274925 -14.084674\n",
       "27    -1.406995 -13.985962\n",
       "28    -1.382691 -14.180301\n",
       "29    -1.330436 -14.038247\n",
       "...         ...        ...\n",
       "12319 -1.061528 -13.505957\n",
       "12320 -1.083420 -13.544744\n",
       "12321 -1.071946 -13.512463\n",
       "12322 -1.085612 -13.499512\n",
       "12323 -1.083938 -13.497822\n",
       "12324 -1.065640 -13.514324\n",
       "12325 -1.055166 -13.494580\n",
       "12326 -1.079917 -13.482646\n",
       "12327 -1.058770 -13.489414\n",
       "12328 -1.048842 -13.515354\n",
       "12329 -1.086797 -13.531395\n",
       "12330 -1.568627 -14.877522\n",
       "12331 -1.344709 -14.725714\n",
       "12332 -1.603326 -15.021224\n",
       "12333 -1.601675 -14.575672\n",
       "12334 -1.595818 -14.426875\n",
       "12335 -1.264544 -14.234146\n",
       "12336 -1.310516 -14.188839\n",
       "12337 -1.169034 -14.199857\n",
       "12338 -1.278854 -14.497911\n",
       "12339 -1.279956 -14.499651\n",
       "12340 -1.178506 -14.005718\n",
       "12341 -1.190674 -14.492249\n",
       "12342 -1.259125 -14.465416\n",
       "12343 -1.256915 -14.308807\n",
       "12344 -1.167905 -14.399468\n",
       "12345 -1.327071 -14.395011\n",
       "12346 -1.277843 -14.402060\n",
       "12347 -1.312950 -14.710134\n",
       "12348 -1.235074 -14.603294\n",
       "\n",
       "[12349 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data_frame, y_data_frame, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestRegressor(random_state= 42)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_clf.predict(X_test)\n",
    "y_pred_pd = pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "error = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(tree_reg.predict(X_test))\n",
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = mean_squared_error(y_pred_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
